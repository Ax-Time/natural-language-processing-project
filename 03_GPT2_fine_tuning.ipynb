{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we tried to fine tuned a gpt2 model with the wow dataset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPT fine-tuning"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We load the data for train, validation and test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import torch\n",
    "\n",
    "# Base_directory\n",
    "base_dir = './wizard_of_wikipedia/'\n",
    "\n",
    "# Load the data\n",
    "with open(base_dir + 'train.json') as f:\n",
    "    train_data = json.load(f)\n",
    "with open(base_dir + 'valid_random_split.json') as f:\n",
    "    valid_data = json.load(f)\n",
    "with open(base_dir + 'test_random_split.json') as f:\n",
    "    test_data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data[0]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We started from this gpt model loaded from huggingface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPTNeoForCausalLM, AutoTokenizer\n",
    "\n",
    "model_id = \"EleutherAI/gpt-neo-125M\"\n",
    "\n",
    "# default_device = 'cpu'\n",
    "default_device = 'mps' # apple silicon\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else default_device)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, padding_side='left')\n",
    "model = GPTNeoForCausalLM.from_pretrained('./model').to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_checked_sentence(utterance):\n",
    "    try:\n",
    "        checked_sentence = list(utterance['checked_sentence'].values())[0]\n",
    "        return 'PASSAGE: ' + checked_sentence + '\\n'\n",
    "    except:\n",
    "        return ''\n",
    "\n",
    "def parse_dialog(dialog):\n",
    "        return '\\n'.join([\n",
    "            f'SPEAKER: {utterance[\"speaker\"]}\\n' + \\\n",
    "            extract_checked_sentence(utterance) + \\\n",
    "            f'TEXT: {utterance[\"text\"]}\\n'\n",
    "        for utterance in dialog])\n",
    "\n",
    "def parse_data(dataset):\n",
    "    return [\n",
    "        f'CHOSEN_TOPIC: {sample[\"chosen_topic\"]}\\n' + \\\n",
    "        f'PERSONA: {sample[\"persona\"]}\\n' + \\\n",
    "        parse_dialog(sample['dialog'])\n",
    "    for sample in dataset]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_parsed = parse_data(train_data)\n",
    "valid_parsed = parse_data(valid_data)\n",
    "test_parsed = parse_data(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "train_parsed = Dataset.from_dict({'text': train_parsed})\n",
    "valid_parsed = Dataset.from_dict({'text': valid_parsed})\n",
    "test_parsed = Dataset.from_dict({'text': test_parsed})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import DatasetDict\n",
    "\n",
    "data = DatasetDict()\n",
    "data['train'] = train_parsed\n",
    "data['validation'] = valid_parsed\n",
    "data['test'] = test_parsed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 19/19 [00:12<00:00,  1.48ba/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.70ba/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  2.22ba/s]\n"
     ]
    }
   ],
   "source": [
    "def tokenize_function(examples):\n",
    "    input_encodings = tokenizer(examples[\"text\"], padding=True, truncation=True)\n",
    "    sample = {\n",
    "        'input_ids': input_encodings.input_ids\n",
    "    }\n",
    "    return sample\n",
    "\n",
    "tokenized_data = data.map(tokenize_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForLanguageModeling\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer, mlm=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    \"cooler_trainer_name\", \n",
    "    evaluation_strategy=\"steps\",\n",
    "    per_device_train_batch_size=1,\n",
    "    gradient_accumulation_steps=4,\n",
    "    learning_rate=6.25e-5,\n",
    "    lr_scheduler_type=\"linear\",\n",
    "    per_device_eval_batch_size=1,\n",
    "    use_mps_device=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model, \n",
    "    args=training_args, \n",
    "    train_dataset=tokenized_data['train'], \n",
    "    eval_dataset=tokenized_data['validation'],\n",
    "    data_collator=data_collator\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train(resume_from_checkpoint=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CHOSEN_TOPIC: Science fiction\n",
      "PERSONA: i enjoy movies about aliens invading the earth.\n",
      "SPEAKER: 0_Wizard\n",
      "PASSAGE: Science fiction (often shortened to SF or sci-fi) is a genre of speculative fiction, typically dealing with imaginative concepts such as futuristic science and technology, space travel, time travel, faster than light travel, parallel universes, and extraterrestrial life.\n",
      "TEXT: I think science fiction is an amazing genre for anything. Future science, technology, time travel, FTL travel, they're all such interesting concepts.\n",
      "\n",
      "SPEAKER: 1_Apprentice\n",
      "TEXT: I'm a huge fan of science fiction myself! \n",
      "\n",
      "SPEAKER: 0_Wizard\n",
      "PASSAGE: Science fiction films have often been used to focus on political or social issues, and to explore philosophical issues like the human condition.\n",
      "TEXT: Awesome! I really love how sci-fi storytellers focus on political/social/philosophical issues that would still be around even in the future. Makes them relatable.\n",
      "\n",
      "SPEAKER: 1_Apprentice\n",
      "TEXT: I agree. One of my favorite forms of science fiction is anything related to time travel! I find it fascinating.\n",
      "\n",
      "SPEAKER: 0_Wizard\n",
      "PASSAGE: The central premise for these stories oftentimes involves changing history, either intentionally or by accident, and the ways by which altering the past changes the future and creates an altered present or future for the time traveler when they return home.\n",
      "TEXT: It's not quite sci-fi, but my favorite version of time travel is in Harry Potter and the Prisoner of Azkaban. Breaks zero logical rules.\n",
      "\n",
      "SPEAKER: 1_Apprentice\n",
      "TEXT: And that's difficult to do when dealing with time travel. I actually haven't seen the latest Harry Potter movies. Guess it's time to check them out!\n",
      "\n",
      "SPEAKER: 0_Wizard\n",
      "PASSAGE: Science fiction often explores the potential consequences of scientific and other innovations, and has been called a \"literature of ideas\".\n",
      "TEXT: If you really want a look at the potential negative consequences of scientific innovation, what you should check out is the TV show Fringe. Incredibly well written.\n",
      "\n",
      "SPEAKER: 1_Apprentice\n",
      "TEXT: Thank you for the suggestion, I will definitely check it out!\n",
      "\n",
      "SPEAKER: 0_Wizard\n",
      "PASSAGE: no_passages_used\n",
      "TEXT: It blends science fiction and paranormal/psychological/MK Ultra type stuff together, but it's science fiction at its core.\n",
      "\n",
      "SPEAKER: 1_Apprentice\n",
      "TEXT: Always looking for more science fiction to digest!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(train_parsed[0]['text'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test the model on a some sentences of the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GENERATION_LENGTH = 200\n",
    "\n",
    "test_index = [0, 5, 6, 12, 13, 19, 50]\n",
    "\n",
    "outputs = []\n",
    "\n",
    "for i in test_index:\n",
    "    train = train_parsed[i]['text']\n",
    "    split_train = train.split('\\n')\n",
    "    input = '\\n'.join(split_train[:5])\n",
    "    encoded_input = tokenizer.encode(input, return_tensors=\"pt\")\n",
    "    encoded_output = model.generate(encoded_input, do_sample=True, max_length=GENERATION_LENGTH, top_p=0.95, temperature=0.85)\n",
    "    decoded_output = tokenizer.decode(encoded_output[0], skip_special_tokens=True)\n",
    "    output = decoded_output.split('\\n')\n",
    "    topic_output = []\n",
    "    topic_output.append(output[0])\n",
    "    topic_output.append(output[2])\n",
    "    topic_output.append(output[4])\n",
    "    topic_output.append(output[6:8])\n",
    "    outputs.append(topic_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CHOSEN_TOPIC: Science fiction\n",
      "SPEAKER: 0_Wizard\n",
      "TEXT: I think science fiction is an amazing genre for anything. Future science, technology, time travel, FTL travel, they're all such interesting concepts. I don't want to be a total science person.\n",
      "['SPEAKER: 1_Apprentice', 'TEXT: I agree, I love science fiction. Have you ever watched it?']\n",
      "\n",
      "\n",
      "CHOSEN_TOPIC: Romance (love)\n",
      "SPEAKER: 0_Wizard\n",
      "TEXT: I don't know how to be romantic. I have trouble expressing emotional attraction. Do you enjoy romance?\n",
      "['SPEAKER: 1_Apprentice', 'TEXT: I love romance. It is the easiest way to get into love']\n",
      "\n",
      "\n",
      "CHOSEN_TOPIC: Krav Maga\n",
      "SPEAKER: 0_Wizard\n",
      "TEXT: Hello. I hope you might enjoy or know something about Krav Maga? It's a sport in which people try to keep their balance. Do you like it?\n",
      "['SPEAKER: 1_Apprentice', 'TEXT: I enjoy it too! I love krav maga!']\n",
      "\n",
      "\n",
      "CHOSEN_TOPIC: The Hershey Company\n",
      "SPEAKER: 0_Wizard\n",
      "TEXT: Hi there, I love chocolate, my favorite brand of chocolate is Hershey coming from my local city of Pennsylvania! \n",
      "['SPEAKER: 1_Apprentice', \"TEXT: I like Hershey's Chocolate it is great\"]\n",
      "\n",
      "\n",
      "CHOSEN_TOPIC: Divorce\n",
      "SPEAKER: 0_Wizard\n",
      "TEXT: Divorce laws vary by state and in most countries. Most require a court and a legal process with issues of alimony and child support and visitation. I was divorced in 2001. \n",
      "['SPEAKER: 1_Apprentice', \"TEXT: That is very long ago. I've heard that it is very common in some states to have people divorce and that people get more out of it than they would if they were married. What makes you want to get divorced?\"]\n",
      "\n",
      "\n",
      "CHOSEN_TOPIC: Fly fishing\n",
      "SPEAKER: 0_Wizard\n",
      "TEXT: I would like to try fly fishing  in fresh or salt water. So i will need to learn to fly the fly, its hard. Do you like to fly?\n",
      "['SPEAKER: 1_Apprentice', \"TEXT: I enjoy it also, but i don't know much about it. What kind of fish do you like to fish? \"]\n",
      "\n",
      "\n",
      "CHOSEN_TOPIC: Marathon\n",
      "SPEAKER: 0_Wizard\n",
      "TEXT: I love running marathons, all 26 miles! I'm a runner too! \n",
      "['SPEAKER: 1_Apprentice', 'TEXT: Me too. I have won a lot of marathons though, but only in part due to my running. What are some of your favorite runners?']\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for output in outputs:\n",
    "    for elem in output:\n",
    "        print(elem)\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['CHOSEN_TOPIC: AlphaZero', 'PERSONA: I am a chess enthusiast.', 'SPEAKER: 0_Wizard', 'PASSAGE: ', 'AlphaZero is a computer program developed by artificial intelligence research company DeepMind to master the games of chess, shogi and go. This algorithm uses an approach similar to AlphaGo Zero.', '', 'On December 5, 2017, the DeepMind team released a preprint paper introducing AlphaZero, which within 24 hours of training achieved a superhuman level of play in these three games by defeating world-champion programs Stockfish, Elmo, and the three-day version of AlphaGo Zero. In each case it made use of custom tensor processing units (TPUs) that the Google programs were optimized to use.[1] AlphaZero was trained solely via self-play using 5,000 first-generation TPUs to generate the games and 64 second-generation TPUs to train the neural networks, all in parallel, with no access to opening books or endgame tables. After four hours of training, DeepMind estimated AlphaZero was playing chess at a higher Elo rating than Stockfish 8; after nine hours of training, the algorithm defeated Stockfish 8 in a time-controlled 100-game tournament (28 wins, 0 losses, and 72 draws).[1][2][3] The trained algorithm played on a single machine with four TPUs.', '', \"DeepMind's paper on AlphaZero was published in the journal Science on 7 December 2018;[4] however, the AlphaZero program itself has not been made available to the public.[5] In 2019, DeepMind published a new paper detailing MuZero, a new algorithm able to generalise AlphaZero's work, playing both Atari and board games without knowledge of the rules or representations of the game.[6]\", '', 'TEXT: AlphaZero is so impressive! When I first read it was far better at playing than the world champion I was astonished!', '', 'SPEAKER: 0_Wizard', 'PASSAGE: It is designed to have the \"strength\" and \"delight\" of playing against a very large and powerful opponent, but with the added bonus of providing speed and improved on-court control, which allows the winner to play more effectively against the eventual opponents with a much smaller opponent.', 'TEXT: It was so good at playing against a large opponent that it made the world champions the most powerful of the players.', '', 'SPEAKER: 1_Apprentice', 'TEXT: Yes, and it is one of the main reasons why I like to play chess. What is your favorite chess game?', '', 'SPEAKER: 0_Wizard', 'PASSAGE: The game is played on a single machine, with the goal of picking the best pieces from a set of 20 pieces that have a score at least 50% of the time, and the board is']\n"
     ]
    }
   ],
   "source": [
    "topic = 'AlphaZero'\n",
    "passage = \"\"\"\n",
    "AlphaZero is a computer program developed by artificial intelligence research company DeepMind to master the games of chess, shogi and go. This algorithm uses an approach similar to AlphaGo Zero.\n",
    "\n",
    "On December 5, 2017, the DeepMind team released a preprint paper introducing AlphaZero, which within 24 hours of training achieved a superhuman level of play in these three games by defeating world-champion programs Stockfish, Elmo, and the three-day version of AlphaGo Zero. In each case it made use of custom tensor processing units (TPUs) that the Google programs were optimized to use.[1] AlphaZero was trained solely via self-play using 5,000 first-generation TPUs to generate the games and 64 second-generation TPUs to train the neural networks, all in parallel, with no access to opening books or endgame tables. After four hours of training, DeepMind estimated AlphaZero was playing chess at a higher Elo rating than Stockfish 8; after nine hours of training, the algorithm defeated Stockfish 8 in a time-controlled 100-game tournament (28 wins, 0 losses, and 72 draws).[1][2][3] The trained algorithm played on a single machine with four TPUs.\n",
    "\n",
    "DeepMind's paper on AlphaZero was published in the journal Science on 7 December 2018;[4] however, the AlphaZero program itself has not been made available to the public.[5] In 2019, DeepMind published a new paper detailing MuZero, a new algorithm able to generalise AlphaZero's work, playing both Atari and board games without knowledge of the rules or representations of the game.[6]\n",
    "\"\"\"\n",
    "text = 'AlphaZero is so impressive! When I first read it was far better at playing than the world champion I was astonished!\\n'\n",
    "input = f'CHOSEN_TOPIC: {topic}\\n' \\\n",
    "    'PERSONA: I am a chess enthusiast.\\n' \\\n",
    "    'SPEAKER: 0_Wizard\\n' \\\n",
    "    f'PASSAGE: {passage}\\n' \\\n",
    "    f'TEXT: {text}'\n",
    "\n",
    "encoded_input = tokenizer.encode(input, return_tensors=\"pt\")\n",
    "encoded_output = model.generate(encoded_input, do_sample=True, max_length=GENERATION_LENGTH*3, top_p=0.95, temperature=0.80)\n",
    "decoded_output = tokenizer.decode(encoded_output[0], skip_special_tokens=True)\n",
    "output = decoded_output.split('\\n')\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEXT: AlphaZero is so impressive! When I first read it was far better at playing than the world champion I was astonished!\n",
      "TEXT: It was so good at playing against a large opponent that it made the world champions the most powerful of the players.\n"
     ]
    }
   ],
   "source": [
    "print(output[10])\n",
    "print(output[14])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
