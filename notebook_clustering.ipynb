{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Read json file\n",
    "with open('wizard_of_wikipedia/data.json') as f:\n",
    "    data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The data is a list of conversations.\n",
      "Each conversation contains the attributes:\n",
      "- chosen_topic\n",
      "- persona\n",
      "- wizard_eval\n",
      "- dialog\n",
      "- chosen_topic_passage\n",
      "The dialog object contains a list of objects, each one with the following attributes:\n",
      "- speaker\n",
      "- text\n",
      "- checked_sentence\n",
      "- checked_passage\n",
      "- retrieved_passages\n",
      "- retrieved_topics\n",
      "Each dialog contains at least 4-5 utterances.\n"
     ]
    }
   ],
   "source": [
    "print('The data is a list of conversations.')\n",
    "print('Each conversation contains the attributes:')\n",
    "for att in data[0].keys():\n",
    "    print('-', att)\n",
    "print('The dialog object contains a list of objects, each one with the following attributes:')\n",
    "for att in data[0]['dialog'][0].keys():\n",
    "    print('-', att)\n",
    "print('Each dialog contains at least 4-5 utterances.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are a total of 22311 dialogues in the dataset.\n"
     ]
    }
   ],
   "source": [
    "n_dialogues = len(data)\n",
    "print('There are a total of', n_dialogues, 'dialogues in the dataset.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are a total of 201999 utterances in the dataset.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "n_utterances = np.sum(np.array([len(d['dialog']) for d in data]))\n",
    "print('There are a total of', n_utterances, 'utterances in the dataset.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average number of utterances per dialogue is 9.053785128411993\n"
     ]
    }
   ],
   "source": [
    "print('The average number of utterances per dialogue is', n_utterances/n_dialogues)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "dialogues = [d['dialog'] for d in data]\n",
    "texts = []\n",
    "for dialog in dialogues:\n",
    "    for obj in dialog:\n",
    "        texts.append(obj['text'])\n",
    "full_text = \" \".join(texts).lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/giacomocartechini/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/giacomocartechini/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = nltk.Counter(full_text.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of words in the dataset is 3305777.\n",
      "The vocabulary size is 106894.\n"
     ]
    }
   ],
   "source": [
    "print(f'The number of words in the dataset is {sum(c.values())}.')\n",
    "print(f'The vocabulary size is {len(c)}.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The most common words excluding stopwords are: [('like', 25439), ('know', 16293), ('love', 14229), ('think', 11007), ('really', 10816), ('one', 10177), ('would', 9869), (\"that's\", 8427), ('also', 8170), (\"i'm\", 8114)]\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stopwords.words('english')\n",
    "\n",
    "# Get the text without stopwords\n",
    "c_no_stopwords = nltk.Counter({key: c[key] for key in c if key not in stopwords.words('english')})\n",
    "\n",
    "print(f'The most common words excluding stopwords are: {c_no_stopwords.most_common(10)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of different topics is 1365.\n"
     ]
    }
   ],
   "source": [
    "n_topics = len(set([d['chosen_topic'] for d in data]))\n",
    "print(f'The number of different topics is {n_topics}.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the dataset with full dialogues and topics\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "dataset = []\n",
    "tokenizer = RegexpTokenizer(r'\\w+') # This tokenizer removes punctuation\n",
    "for d in data:\n",
    "    topic = d['chosen_topic']\n",
    "    dialogue = [obj['text'] for obj in d['dialog']]\n",
    "    dialogue = \" \".join(dialogue).lower()\n",
    "    tokens = tokenizer.tokenize(dialogue)\n",
    "    # Remove stopwords\n",
    "    tokens = [t for t in tokens if t not in stopwords.words('english')]\n",
    "    c = nltk.Counter(tokens)\n",
    "    dataset.append((c, topic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# def dump_dataset_clustering():\n",
    "#     with open('dumps/clustering/clustering_dataset.pkl', 'wb') as f:\n",
    "#         pickle.dump(dataset, f)\n",
    "\n",
    "def load_dataset_clustering():\n",
    "    with open('dumps/clustering/clustering_dataset.pkl', 'rb') as f:\n",
    "        dataset = pickle.load(f)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dump the dataset\n",
    "# dump_dataset_clustering()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "dataset = load_dataset_clustering()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Train a word2vec model\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "# # Create a list containing all the text pieces in each dialogue of the dataset\n",
    "# texts = []\n",
    "# for d in data:\n",
    "#     dialogue = [obj['text'] for obj in d['dialog']]\n",
    "#     texts += dialogue\n",
    "\n",
    "# # Remove numbers and special characters\n",
    "# texts = [re.sub('[^A-Za-z]+', ' ', text) for text in texts]\n",
    "\n",
    "# # Tokenize the text\n",
    "# tokenizer = RegexpTokenizer(r'\\w+')\n",
    "# texts = [tokenizer.tokenize(text) for text in texts]\n",
    "\n",
    "# # Lower case\n",
    "# texts = [[word.lower() for word in text] for text in texts]\n",
    "\n",
    "# # Remove stopwords\n",
    "# stopwords = set(stopwords.words('english'))\n",
    "# texts = [[word for word in text if word not in stopwords] for text in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and dump the model\n",
    "# model = Word2Vec(texts, vector_size=50, window=5, min_count=1, workers=8)\n",
    "# model.save('dumps/clustering/word2vec.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model\n",
    "model = Word2Vec.load('dumps/clustering/word2vec.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create tf-idf matrices\n",
    "\n",
    "# def calculate_tf(counter, word):\n",
    "#     return np.log(1 + counter[word] / sum(counter.values()))\n",
    "\n",
    "# def calculate_idf(dataset, word):\n",
    "#     return np.log(len(dataset) / (1 + sum(([1 for d in dataset if word in d[0].keys()])))) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate tf\n",
    "# tf = {(i, word): calculate_tf(dataset[i][0], word) for i in range(len(dataset)) for word in dataset[i][0].keys()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate idf\n",
    "# counter = nltk.Counter()\n",
    "# for sample in dataset:\n",
    "#     counter = counter + sample[0]\n",
    "# idf = {word: calculate_idf(dataset, word) for word in counter.keys()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dump tf and idf matrices\n",
    "# with open('dumps/clustering/tf.pkl', 'wb') as f:\n",
    "#     pickle.dump(tf, f)\n",
    "# with open('dumps/clustering/idf.pkl', 'wb') as f:\n",
    "#     pickle.dump(idf, f)\n",
    "\n",
    "# Load tf and idf matrices\n",
    "with open('dumps/clustering/tf.pkl', 'rb') as f:\n",
    "    tf = pickle.load(f)\n",
    "with open('dumps/clustering/idf.pkl', 'rb') as f:\n",
    "    idf = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"Key '1995' not present\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[26], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m tf_idf \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray([tf[(i, word)] \u001b[39m*\u001b[39m idf[word] \u001b[39mfor\u001b[39;00m word \u001b[39min\u001b[39;00m dataset[i][\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mkeys()])\n\u001b[1;32m      6\u001b[0m \u001b[39m# Get the word embedding for each word in the dialogue\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m embeddings \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray([model\u001b[39m.\u001b[39mwv[word] \u001b[39mfor\u001b[39;00m word \u001b[39min\u001b[39;00m dataset[i][\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mkeys()])\n\u001b[1;32m      8\u001b[0m \u001b[39m# Calculate the weighted average of the word embeddings\u001b[39;00m\n\u001b[1;32m      9\u001b[0m weighted_average \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39maverage(embeddings, axis\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m, weights\u001b[39m=\u001b[39mtf_idf)\n",
      "Cell \u001b[0;32mIn[26], line 7\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      5\u001b[0m tf_idf \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray([tf[(i, word)] \u001b[39m*\u001b[39m idf[word] \u001b[39mfor\u001b[39;00m word \u001b[39min\u001b[39;00m dataset[i][\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mkeys()])\n\u001b[1;32m      6\u001b[0m \u001b[39m# Get the word embedding for each word in the dialogue\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m embeddings \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray([model\u001b[39m.\u001b[39;49mwv[word] \u001b[39mfor\u001b[39;00m word \u001b[39min\u001b[39;00m dataset[i][\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mkeys()])\n\u001b[1;32m      8\u001b[0m \u001b[39m# Calculate the weighted average of the word embeddings\u001b[39;00m\n\u001b[1;32m      9\u001b[0m weighted_average \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39maverage(embeddings, axis\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m, weights\u001b[39m=\u001b[39mtf_idf)\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/lib/python3.10/site-packages/gensim/models/keyedvectors.py:403\u001b[0m, in \u001b[0;36mKeyedVectors.__getitem__\u001b[0;34m(self, key_or_keys)\u001b[0m\n\u001b[1;32m    389\u001b[0m \u001b[39m\"\"\"Get vector representation of `key_or_keys`.\u001b[39;00m\n\u001b[1;32m    390\u001b[0m \n\u001b[1;32m    391\u001b[0m \u001b[39mParameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    400\u001b[0m \n\u001b[1;32m    401\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    402\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(key_or_keys, _KEY_TYPES):\n\u001b[0;32m--> 403\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mget_vector(key_or_keys)\n\u001b[1;32m    405\u001b[0m \u001b[39mreturn\u001b[39;00m vstack([\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_vector(key) \u001b[39mfor\u001b[39;00m key \u001b[39min\u001b[39;00m key_or_keys])\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/lib/python3.10/site-packages/gensim/models/keyedvectors.py:446\u001b[0m, in \u001b[0;36mKeyedVectors.get_vector\u001b[0;34m(self, key, norm)\u001b[0m\n\u001b[1;32m    422\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_vector\u001b[39m(\u001b[39mself\u001b[39m, key, norm\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m):\n\u001b[1;32m    423\u001b[0m     \u001b[39m\"\"\"Get the key's vector, as a 1D numpy array.\u001b[39;00m\n\u001b[1;32m    424\u001b[0m \n\u001b[1;32m    425\u001b[0m \u001b[39m    Parameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    444\u001b[0m \n\u001b[1;32m    445\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 446\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mget_index(key)\n\u001b[1;32m    447\u001b[0m     \u001b[39mif\u001b[39;00m norm:\n\u001b[1;32m    448\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfill_norms()\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/lib/python3.10/site-packages/gensim/models/keyedvectors.py:420\u001b[0m, in \u001b[0;36mKeyedVectors.get_index\u001b[0;34m(self, key, default)\u001b[0m\n\u001b[1;32m    418\u001b[0m     \u001b[39mreturn\u001b[39;00m default\n\u001b[1;32m    419\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 420\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mKey \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mkey\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m not present\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;31mKeyError\u001b[0m: \"Key '1995' not present\""
     ]
    }
   ],
   "source": [
    "cluster = []\n",
    "\n",
    "for i in range(len(dataset)):\n",
    "    # Get the tf-idf vector for each word in the dialogue\n",
    "    tf_idf = np.array([tf[(i, word)] * idf[word] for word in dataset[i][0].keys()])\n",
    "    # Get the word embedding for each word in the dialogue\n",
    "    embeddings = np.array([model.wv[word] for word in dataset[i][0].keys()])\n",
    "    # Calculate the weighted average of the word embeddings\n",
    "    weighted_average = np.average(embeddings, axis=0, weights=tf_idf)\n",
    "    # Append the weighted average to the cluster\n",
    "    cluster.append((weighted_average, dataset[i][1]))\n",
    "\n",
    "cluster[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[0][0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
