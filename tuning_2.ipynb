{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Base_directory\n",
    "base_dir = './wizard_of_wikipedia/'\n",
    "\n",
    "# Load the data\n",
    "with open(base_dir + 'train.json') as f:\n",
    "    train_data = json.load(f)\n",
    "with open(base_dir + 'valid_random_split.json') as f:\n",
    "    valid_data = json.load(f)\n",
    "with open(base_dir + 'test_random_split.json') as f:\n",
    "    test_data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import GPTNeoForCausalLM, AutoTokenizer\n",
    "\n",
    "model_id = \"EleutherAI/gpt-neo-125M\"\n",
    "\n",
    "default_device = 'cpu' # 'mps'  (for apple silicon gpu)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else default_device)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, padding_side='left')\n",
    "model = GPTNeoForCausalLM.from_pretrained(model_id).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_checked_sentence(utterance):\n",
    "    try:\n",
    "        checked_sentence = list(utterance['checked_sentence'].values())[0]\n",
    "        return 'PASSAGE: ' + checked_sentence + '\\n'\n",
    "    except:\n",
    "        return ''\n",
    "\n",
    "def parse_dialog(dialog):\n",
    "        return '\\n'.join([\n",
    "            f'SPEAKER: {utterance[\"speaker\"]}\\n' + \\\n",
    "            extract_checked_sentence(utterance) + \\\n",
    "            f'TEXT: {utterance[\"text\"]}\\n'\n",
    "        for utterance in dialog])\n",
    "\n",
    "def parse_data(dataset):\n",
    "    return [\n",
    "        f'CHOSEN_TOPIC: {sample[\"chosen_topic\"]}\\n' + \\\n",
    "        f'PERSONA: {sample[\"persona\"]}\\n' + \\\n",
    "        parse_dialog(sample['dialog'])\n",
    "    for sample in dataset]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_parsed = parse_data(train_data)\n",
    "valid_parsed = parse_data(valid_data)\n",
    "test_parsed = parse_data(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from parlai.tasks.convai2.build import build\n",
    "\n",
    "# build({'datapath': './data/'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def parse_pc(path):\n",
    "#     # Open file\n",
    "#     with open('./data/ConvAI2/train_self_original.txt') as f:\n",
    "#         # Read raw file lines\n",
    "#         data = [line.strip() for line in f]\n",
    "#     # Data set container\n",
    "#     persona_chat = list()\n",
    "#     # Now we iterate through lines and build the data set\n",
    "#     for line in data:\n",
    "#         # Split line data from initial index\n",
    "#         line_idx, line_data = line.split(' ', 1)\n",
    "#         # Check if new conversation is started\n",
    "#         if line_idx == '1':\n",
    "#             # Add new empthy dialogue in data set\n",
    "#             persona_chat.append(\n",
    "#                 {'persona_a': list(), 'persona_b': list(), 'utterances': list()}\n",
    "#             )\n",
    "#         # If the line is from Speaker A persona\n",
    "#         if line_data.startswith('your persona: '):\n",
    "#             # Append it to Persona A\n",
    "#             persona_chat[-1]['persona_a'].append(line_data[len('your persona: '):])\n",
    "#         # Else if the line is from Speaker B persona\n",
    "#         elif line_data.startswith('partner\\'s persona: '):\n",
    "#             # Append it to Persona B\n",
    "#             persona_chat[-1]['persona_b'].append(line_data[len('partner\\'s persona: '):])\n",
    "#         # Else the line is a regular dialogue line\n",
    "#         else:\n",
    "#             # Split utterances from distractors and separate A and B\n",
    "#             utt_a, utt_b = line_data.split('\\t\\t')[0].split('\\t')\n",
    "#             # Append to dialogue utterances\n",
    "#             persona_chat[-1]['utterances'].append(\n",
    "#                 {'speaker': 'A', 'text': utt_a}\n",
    "#             )\n",
    "#             persona_chat[-1]['utterances'].append(\n",
    "#                 {'speaker': 'B', 'text': utt_b}\n",
    "#             )\n",
    "            \n",
    "#     return persona_chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training_data = parse_pc('./data/ConvAI2/train_both_original.txt')\n",
    "# validation_data = parse_pc('./data/ConvAI2/valid_both_original.txt')\n",
    "\n",
    "# training_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def sample_to_string(sample, eos_token):\n",
    "#     # Join strings of Persona A\n",
    "#     persona_a = ' '.join(sample['persona_a'])\n",
    "#     # Join strings of Persona B\n",
    "#     persona_b = ' '.join(sample['persona_b'])\n",
    "#     # Join dialogue strings\n",
    "#     dialogue = eos_token.join(f\"{utterance['speaker']}: {utterance['text']}\" for utterance in sample['utterances'])\n",
    "#     # Build the dialogue string\n",
    "#     dialogue_string = f\"Persona A: {persona_a}{eos_token}Persona B: {persona_b}{eos_token}{dialogue}{eos_token}\"\n",
    "    \n",
    "#     return dialogue_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training_data_str = [sample_to_string(dialogue, tokenizer.eos_token) for dialogue in training_data]\n",
    "# validation_data_str = [sample_to_string(dialogue, tokenizer.eos_token) for dialogue in validation_data]\n",
    "\n",
    "# training_data_str[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from datasets import Dataset\n",
    "\n",
    "# train_data = Dataset.from_dict({'text': training_data_str})\n",
    "# valid_data = Dataset.from_dict({'text': validation_data_str})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "train_parsed = Dataset.from_dict({'text': train_parsed})\n",
    "valid_parsed = Dataset.from_dict({'text': valid_parsed})\n",
    "test_parsed = Dataset.from_dict({'text': test_parsed})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import DatasetDict\n",
    "\n",
    "data = DatasetDict()\n",
    "data['train'] = train_parsed\n",
    "data['validation'] = valid_parsed\n",
    "data['test'] = valid_parsed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 19/19 [00:04<00:00,  4.19ba/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  4.45ba/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  4.47ba/s]\n"
     ]
    }
   ],
   "source": [
    "def tokenize_function(examples):\n",
    "    input_encodings = tokenizer(examples[\"text\"], padding=True, truncation=True)\n",
    "    sample = {\n",
    "        'input_ids': input_encodings.input_ids\n",
    "    }\n",
    "    return sample\n",
    "\n",
    "tokenized_data = data.map(tokenize_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForLanguageModeling\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer, mlm=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    \"cooler_trainer_name\", \n",
    "    evaluation_strategy=\"steps\",\n",
    "    per_device_train_batch_size=8,\n",
    "    gradient_accumulation_steps=4,\n",
    "    learning_rate=6.25e-5,\n",
    "    lr_scheduler_type=\"linear\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model, \n",
    "    args=training_args, \n",
    "    train_dataset=tokenized_data['train'], \n",
    "    eval_dataset=tokenized_data['validation'],\n",
    "    data_collator=data_collator\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/giacomocartechini/Desktop/projects/natural-language-processing-project/.env/lib/python3.10/site-packages/transformers/optimization.py:407: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "  0%|          | 0/1728 [00:00<?, ?it/s]You're using a GPT2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "[E thread_pool.cpp:109] Exception in thread pool task: mutex lock failed: Invalid argument\n",
      "[E thread_pool.cpp:109] Exception in thread pool task: mutex lock failed: Invalid argument\n",
      "[E thread_pool.cpp:109] Exception in thread pool task: mutex lock failed: Invalid argument\n",
      "[E thread_pool.cpp:109] Exception in thread pool task: mutex lock failed: Invalid argument\n"
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
